{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":10284807,"sourceType":"datasetVersion","datasetId":6364507},{"sourceId":10334179,"sourceType":"datasetVersion","datasetId":6398841}],"dockerImageVersionId":30840,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Importing Required Libraries","metadata":{}},{"cell_type":"markdown","source":"\n \nBelow cell imports essential libraries for object detection using the DETR (DEtection TRansformer) model with PyTorch Lightning. The key libraries include:  \n\n- `numpy` and `pandas` for data handling  \n- `torch` and `torchvision` for deep learning operations  \n- `pytorch_lightning` for streamlined training and evaluation  \n- `transformers` for utilizing the DETR model and its image processor  \n- `torch.utils.data.DataLoader` for batching data  \n- `requests` and `zipfile` for handling dataset downloads and extraction  \n- `os` for file path management  \n","metadata":{}},{"cell_type":"code","source":"\nimport numpy as np \nimport pandas as pd \nimport pytorch_lightning as pl\nfrom transformers import DetrForObjectDetection\nimport torch\nimport os\nimport zipfile\nimport requests\nimport torchvision\nfrom transformers import DetrImageProcessor\nfrom torch.utils.data import DataLoader\nfrom pytorch_lightning import Trainer\nfrom pytorch_lightning.callbacks import ModelCheckpoint, LearningRateMonitor\nfrom transformers import DetrForObjectDetection, DetrImageProcessor\nfrom PIL import Image\nimport torch\nimport matplotlib.pyplot as plt\nimport requests\nimport requests\nimport torch\nfrom PIL import Image\nimport matplotlib.pyplot as plt\nimport json\nimport os\nimport torch\nfrom transformers import DetrForObjectDetection, DetrImageProcessor\nimport cv2\nimport torch\nimport numpy as np\nfrom tqdm import tqdm\nfrom transformers import DetrForObjectDetection, DetrImageProcessor","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Downloading and Extracting the Golf Ball Dataset","metadata":{}},{"cell_type":"markdown","source":"\nThis cell downloads the golf ball dataset from Roboflow and extracts it for further use.  \n\n- The dataset is publicly available and downloaded using the `requests` library.  \n- The dataset is saved as a zip file at `/content/golf_ball_dataset.zip`.  \n- The zip file is then extracted to `/content/golf_ball_dataset` using the `zipfile` module.  \n\nThis ensures that the dataset is ready for preprocessing and model training.","metadata":{}},{"cell_type":"code","source":"\n\ndataset_url = \"https://universe.roboflow.com/ds/nQn3RG98UA?key=elxUi2Hqxa\"\noutput_path = \"/content/golf_ball_dataset.zip\"\n\nprint(\"Downloading dataset...\")\n\nresponse = requests.get(dataset_url)\nwith open(output_path, \"wb\") as file:\n    file.write(response.content)\n\ndataset_dir = \"/content/golf_ball_dataset\"\nprint(\"Extracting dataset...\")\nwith zipfile.ZipFile(output_path, \"r\") as zip_ref:\n    zip_ref.extractall(dataset_dir)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-31T11:08:05.103734Z","iopub.execute_input":"2025-01-31T11:08:05.104026Z","iopub.status.idle":"2025-01-31T11:08:24.382450Z","shell.execute_reply.started":"2025-01-31T11:08:05.104004Z","shell.execute_reply":"2025-01-31T11:08:24.381435Z"}},"outputs":[{"name":"stdout","text":"Downloading dataset...\nExtracting dataset...\n","output_type":"stream"}],"execution_count":3},{"cell_type":"markdown","source":"#  Custom COCO Dataset Class for Object Detection","metadata":{}},{"cell_type":"markdown","source":"\nThis cell defines a custom dataset class, `CocoDetection`, which extends `torchvision.datasets.CocoDetection` to work with the DETR model.  \n\n- The dataset loads images and annotations from the COCO format (`_annotations.coco.json`).  \n- The `DetrImageProcessor` is used to preprocess images and annotations.  \n- In the `__getitem__` method:  \n  - The image and corresponding annotations are retrieved.  \n  - The annotations are structured in a dictionary format (`image_id`, `annotations`).  \n  - The processor encodes the image and annotations into tensors.  \n  - The processed pixel values and labels are returned.  \n\nThis class is essential for loading and preparing data for training the DETR object detection model.","metadata":{}},{"cell_type":"code","source":"\n\nclass CocoDetection(torchvision.datasets.CocoDetection):\n    def __init__(self, img_folder, processor, train=True):\n        ann_file = os.path.join(img_folder, \"_annotations.coco.json\")\n        super().__init__(img_folder, ann_file)\n        self.processor = processor\n\n    def __getitem__(self, idx):\n        img, target = super().__getitem__(idx)\n        image_id = self.ids[idx]\n        target = {\"image_id\": image_id, \"annotations\": target}\n        encoding = self.processor(images=img, annotations=target, return_tensors=\"pt\")\n        pixel_values = encoding[\"pixel_values\"].squeeze()\n        labels = encoding[\"labels\"][0]\n        return pixel_values, labels\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-31T11:10:10.329803Z","iopub.execute_input":"2025-01-31T11:10:10.330110Z","iopub.status.idle":"2025-01-31T11:10:10.335895Z","shell.execute_reply.started":"2025-01-31T11:10:10.330086Z","shell.execute_reply":"2025-01-31T11:10:10.335063Z"}},"outputs":[],"execution_count":4},{"cell_type":"markdown","source":"# Dataset Preparation and DataLoader Setup","metadata":{}},{"cell_type":"markdown","source":"\nThis cell prepares the dataset and creates data loaders for training and validation.  \n\n- The `DetrImageProcessor` from the pre-trained DETR model (`facebook/detr-resnet-50`) is initialized for image and annotation processing.  \n- The dataset directories for training (`train_dir`) and validation (`val_dir`) are set based on the extracted dataset.  \n- The `CocoDetection` class is used to load the training and validation datasets.  \n- A custom `collate_fn` function:  \n  - Pads pixel values using the processor for batch processing.  \n  - Returns `pixel_values`, `pixel_mask`, and `labels` in a structured format.  \n- DataLoaders (`train_loader` and `val_loader`) are created with a batch size of 32, shuffling enabled for training.  \n\nThis ensures efficient data loading for training and evaluating the object detection model.","metadata":{}},{"cell_type":"code","source":"\n\n# Data processor\nprocessor = DetrImageProcessor.from_pretrained(\"facebook/detr-resnet-50\")\n\n# Dataset directories (accordding to COCO Dataset directory structure)\ntrain_dir = os.path.join(dataset_dir, \"train\")\nval_dir = os.path.join(dataset_dir, \"valid\")\n\n# Load datasets\ntrain_dataset = CocoDetection(img_folder=train_dir, processor=processor)\nval_dataset = CocoDetection(img_folder=val_dir, processor=processor)\n\n# DataLoader\ndef collate_fn(batch):\n    pixel_values = [item[0] for item in batch]\n    encoding = processor.pad(pixel_values, return_tensors=\"pt\")\n    labels = [item[1] for item in batch]\n    return {\n        \"pixel_values\": encoding[\"pixel_values\"],\n        \"pixel_mask\": encoding[\"pixel_mask\"],\n        \"labels\": labels,\n    }\n\ntrain_loader = DataLoader(train_dataset, collate_fn=collate_fn, batch_size=32, shuffle=True)\nval_loader = DataLoader(val_dataset, collate_fn=collate_fn, batch_size=32)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-31T11:10:11.585006Z","iopub.execute_input":"2025-01-31T11:10:11.585380Z","iopub.status.idle":"2025-01-31T11:10:12.051158Z","shell.execute_reply.started":"2025-01-31T11:10:11.585351Z","shell.execute_reply":"2025-01-31T11:10:12.050180Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"preprocessor_config.json:   0%|          | 0.00/290 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"855eebc847b0499a9de95c63706cff3f"}},"metadata":{}},{"name":"stdout","text":"loading annotations into memory...\nDone (t=0.17s)\ncreating index...\nindex created!\nloading annotations into memory...\nDone (t=0.02s)\ncreating index...\nindex created!\n","output_type":"stream"}],"execution_count":5},{"cell_type":"markdown","source":"# DETR Model Implementation with PyTorch Lightning","metadata":{}},{"cell_type":"markdown","source":"\nThis cell defines a PyTorch Lightning module for training the DETR (DEtection TRansformer) model for object detection.  \n\n### Key Components:  \n- **`__init__` Method:**  \n  - Initializes a pre-trained `DetrForObjectDetection` model (`facebook/detr-resnet-50`).  \n  - The number of labels is set to `num_classes + 1` to account for the \"no object\" class.  \n  - Learning rate (`lr`), backbone learning rate (`lr_backbone`), and weight decay (`weight_decay`) are set as hyperparameters.  \n\n- **`forward` Method:**  \n  - Passes `pixel_values` and `pixel_mask` through the DETR model to generate predictions.  \n\n- **`training_step` Method:**  \n  - Computes the loss for a batch of images using ground truth labels.  \n  - Logs the training loss for monitoring.  \n\n- **`configure_optimizers` Method:**  \n  - Uses the AdamW optimizer with different learning rates for the backbone and other parameters.  \n  - Parameters in the backbone have a separate learning rate (`lr_backbone`).  \n\nThis class provides an optimized framework for training DETR using PyTorch Lightning, enabling efficient model training and logging.","metadata":{}},{"cell_type":"code","source":"\n\nclass Detr(pl.LightningModule):\n    def __init__(self, lr, lr_backbone, weight_decay, num_classes):\n        super().__init__()\n        self.model = DetrForObjectDetection.from_pretrained(\n            \"facebook/detr-resnet-50\",\n            revision=\"no_timm\",\n            num_labels=num_classes + 1,  # Add one for the \"no object\" class\n            ignore_mismatched_sizes=True\n        )\n        self.lr = lr\n        self.lr_backbone = lr_backbone\n        self.weight_decay = weight_decay\n\n    def forward(self, pixel_values, pixel_mask):\n        return self.model(pixel_values=pixel_values, pixel_mask=pixel_mask)\n\n    def training_step(self, batch, batch_idx):\n        outputs = self.model(\n            pixel_values=batch[\"pixel_values\"],\n            pixel_mask=batch[\"pixel_mask\"],\n            labels=batch[\"labels\"]\n        )\n        loss = outputs.loss\n        self.log(\"training_loss\", loss)\n        return loss\n\n    def configure_optimizers(self):\n        param_dicts = [\n            {\"params\": [p for n, p in self.named_parameters() if \"backbone\" not in n and p.requires_grad]},\n            {\"params\": [p for n, p in self.named_parameters() if \"backbone\" in n and p.requires_grad],\n             \"lr\": self.lr_backbone},\n        ]\n        optimizer = torch.optim.AdamW(param_dicts, lr=self.lr, weight_decay=self.weight_decay)\n        return optimizer\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-31T11:10:14.878244Z","iopub.execute_input":"2025-01-31T11:10:14.878542Z","iopub.status.idle":"2025-01-31T11:10:14.885259Z","shell.execute_reply.started":"2025-01-31T11:10:14.878521Z","shell.execute_reply":"2025-01-31T11:10:14.884278Z"}},"outputs":[],"execution_count":6},{"cell_type":"markdown","source":"#  Model Training Configuration and Execution","metadata":{}},{"cell_type":"markdown","source":"\nThis cell sets up and trains the DETR model using PyTorch Lightning.  \n\n### **Key Steps:**\n- **Hyperparameters Configuration:**  \n  - `num_classes = 1`: The dataset has only one class (golf ball).  \n  - `lr = 1e-4`: Learning rate for model parameters.  \n  - `lr_backbone = 1e-5`: Lower learning rate for the backbone to prevent overfitting.  \n  - `weight_decay = 1e-4`: Regularization term to prevent overfitting.  \n  - `max_epochs = 10`: The model will be trained for 10 epochs.  \n\n- **Model Initialization:**  \n  - The `Detr` class (previously defined) is instantiated with the given hyperparameters.  \n\n- **Callbacks for Training:**  \n  - `ModelCheckpoint`: Saves the best model checkpoint based on `training_loss`.  \n  - `LearningRateMonitor`: Logs the learning rate at each step for analysis.  \n\n- **Trainer Setup:**  \n  - Uses PyTorch Lightning’s `Trainer` for training.  \n  - Runs on a GPU (`accelerator=\"gpu\", devices=1`).  \n\n- **Training Execution:**  \n  - `trainer.fit(model, train_loader)`: Starts model training using the training data.  \n\nThis cell ensures an efficient and structured training process, with automatic checkpointing and learning rate monitoring.","metadata":{}},{"cell_type":"code","source":"\n# parameters\nnum_classes = 1  #  in this case only one class: golf ball\nlr = 1e-4\nlr_backbone = 1e-5\nweight_decay = 1e-4\nmax_epochs = 10\n\n# Initializing  the model\nmodel = Detr(lr=lr, lr_backbone=lr_backbone, weight_decay=weight_decay, num_classes=num_classes)\n\n# Callbacks ( exolained in details in above markdown cell.)\ncheckpoint_callback = ModelCheckpoint(monitor=\"training_loss\", dirpath=\"./checkpoints\", filename=\"detr-{epoch:02d}-{training_loss:.2f}\")\nlr_monitor = LearningRateMonitor(logging_interval=\"step\")\n\n# Trainer instance\ntrainer = Trainer(max_epochs=max_epochs, callbacks=[checkpoint_callback, lr_monitor], accelerator=\"gpu\", devices=1)\n\n# Training the model\ntrainer.fit(model, train_loader)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-31T11:10:16.609218Z","iopub.execute_input":"2025-01-31T11:10:16.609568Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/6.60k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f3153e9a68954defb7617bd3ca6e6d84"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"pytorch_model.bin:   0%|          | 0.00/167M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2bf30b71857046828a8e95a562103fd3"}},"metadata":{}},{"name":"stderr","text":"Some weights of DetrForObjectDetection were not initialized from the model checkpoint at facebook/detr-resnet-50 and are newly initialized because the shapes did not match:\n- class_labels_classifier.weight: found shape torch.Size([92, 256]) in the checkpoint and torch.Size([3, 256]) in the model instantiated\n- class_labels_classifier.bias: found shape torch.Size([92]) in the checkpoint and torch.Size([3]) in the model instantiated\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n/usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/connectors/data_connector.py:425: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=3` in the `DataLoader` to improve performance.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Training: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3f33b95ff43e423faac3350a7765cad6"}},"metadata":{}},{"name":"stderr","text":"The `max_size` parameter is deprecated and will be removed in v4.26. Please specify in `size['longest_edge'] instead`.\n","output_type":"stream"}],"execution_count":null},{"cell_type":"markdown","source":"# Saving the Trained Model and Preprocessor Configuration","metadata":{}},{"cell_type":"markdown","source":"\nThis class-based implementation ensures the **trained DETR model, processor, and preprocessor configuration** are saved correctly after training.\n\n### **Key Features:**\n1. **Class-Based Structure (`ModelSaver`)**:\n   - Encapsulates logic for saving the **model, processor, and configuration**.\n   - Ensures the **correct paths** are used for saving.\n\n2. **Ensures Directories Exist**:\n   - `os.makedirs(self.processor_dir, exist_ok=True)`: Prevents file path errors.\n\n3. **Model and Processor Saving**:\n   - `self.model.save_pretrained(self.model_dir)`: Saves the trained DETR model.\n   - `self.processor.save_pretrained(self.processor_dir)`: Saves the processor.\n\n4. **Explicit Preprocessor Configuration Saving**:\n   - `self._save_processor_config()`: Saves the preprocessor settings (`preprocessor_config.json`) in **JSON format**.\n\n5. **Modular and Reusable**:\n   - Can be reused for multiple models without redundant code.\n\n","metadata":{}},{"cell_type":"code","source":"\n\nclass ModelSaver:\n    def __init__(self, model, processor, model_dir=\"./detr_golf_ball\", processor_dir=\"./processor_golf_ball\"):\n        \"\"\"\n        Initializes the model saver with paths for saving the trained model and processor.\n        \n        Args:\n        - model (DetrForObjectDetection): Trained DETR model.\n        - processor (DetrImageProcessor): Pretrained DETR processor.\n        - model_dir (str): Path to save the trained model.\n        - processor_dir (str): Path to save the processor.\n        \"\"\"\n        self.model = model\n        self.processor = processor\n        self.model_dir = model_dir\n        self.processor_dir = processor_dir\n        self.config_save_path = os.path.join(processor_dir, \"preprocessor_config.json\")\n\n    def save_model_and_processor(self):\n        \"\"\"\n        Saves the trained DETR model and processor, including the preprocessor configuration.\n        \"\"\"\n        # Ensuring  directories exist\n        os.makedirs(self.processor_dir, exist_ok=True)\n\n        # Saving the trained model\n        self.model.save_pretrained(self.model_dir)\n\n        # Saving the processor\n        self.processor.save_pretrained(self.processor_dir)\n\n        # Saving preprocessor configuration\n        self._save_processor_config()\n\n        # Printing confirmation messages\n        print(f\"Model saved to: {self.model_dir}\")\n        print(f\"Processor saved to: {self.processor_dir}\")\n        print(f\"Preprocessor configuration saved to: {self.config_save_path}\")\n\n    def _save_processor_config(self):\n        \"\"\"\n        Saves the preprocessor configuration as a JSON file.\n        \"\"\"\n        preprocessor_config = self.processor.to_json_string()\n        with open(self.config_save_path, \"w\") as f:\n            f.write(preprocessor_config)\n\n\nmodel_save_path = \"./detr_golf_ball\"\nprocessor_save_path = \"./processor_golf_ball\"\n\n# Load trained model and processor\nmodel = DetrForObjectDetection.from_pretrained(model_save_path)\nprocessor = DetrImageProcessor.from_pretrained(processor_save_path)\n\n# Initialize the model saver and save the model & processor\nsaver = ModelSaver(model, processor, model_save_path, processor_save_path)\nsaver.save_model_and_processor()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-29T18:59:33.294381Z","iopub.execute_input":"2024-12-29T18:59:33.294682Z","iopub.status.idle":"2024-12-29T18:59:33.741799Z","shell.execute_reply.started":"2024-12-29T18:59:33.294656Z","shell.execute_reply":"2024-12-29T18:59:33.741036Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"\n# Inference Pipeline for Golf Ball Detection_\n","metadata":{}},{"cell_type":"markdown","source":"\nThis class-based implementation provides a modular approach for detecting golf balls in images using a trained DETR model.  \n\n#### **Key Functionalities:**\n1. **`__init__` Method**:\n   - Loads the pre-trained DETR model and processor.\n   - Moves the model to the specified device (`CPU` or `GPU`).\n   - Defines colors for bounding box visualization.\n\n2. **`load_image` Method**:\n   - Downloads and opens an image from a URL.\n\n3. **`preprocess_image` Method**:\n   - Converts the image into a format suitable for DETR.\n   - Moves tensors to the correct device.\n\n4. **`infer` Method**:\n   - Runs inference on the input image.\n\n5. **`postprocess` Method**:\n   - Filters and extracts bounding boxes, confidence scores, and labels.\n\n6. **`plot_predictions` Method**:\n   - Draws bounding boxes and class labels on the image.\n\n7. **`detect` Method**:\n   - Runs the full detection pipeline: image loading, preprocessing, inference, and visualization.\n   - Prints raw detection results.\n\n#### **Example Usage**:\n- The model and processor are loaded from saved files.\n- The image is fetched from a given URL.\n- The detection pipeline is executed, and results are displayed.\n","metadata":{}},{"cell_type":"code","source":"\nclass GolfBallDetector:\n    def __init__(self, model_path, processor_path, device=\"cuda\" if torch.cuda.is_available() else \"cpu\"):\n        \"\"\"\n        Initializes the Golf Ball Detector.\n        \n        Args:\n        - model_path (str): Path to the saved DETR model.\n        - processor_path (str): Path to the saved image processor.\n        - device (str): Device to run inference on (\"cuda\" or \"cpu\").\n        \"\"\"\n        self.device = device\n        self.model = DetrForObjectDetection.from_pretrained(model_path).to(self.device)\n        self.processor = DetrImageProcessor.from_pretrained(processor_path)\n        self.COLORS = [[0.000, 0.447, 0.741], [0.850, 0.325, 0.098], [0.929, 0.694, 0.125]]\n        self.model.eval()\n\n    def load_image(self, image_url):\n        \"\"\"\n        Downloads and loads an image from a URL.\n        \n        Args:\n        - image_url (str): URL of the image.\n        \n        Returns:\n        - image (PIL Image): Loaded image in RGB format.\n        \"\"\"\n        response = requests.get(image_url, stream=True)\n        image = Image.open(response.raw).convert(\"RGB\")\n        return image\n\n    def preprocess_image(self, image):\n        \"\"\"\n        Prepares the image for DETR model inference.\n        \n        Args:\n        - image (PIL Image): Input image.\n        \n        Returns:\n        - inputs (dict): Processed input tensors.\n        - target_sizes (torch.Tensor): Image size tensor for post-processing.\n        \"\"\"\n        inputs = self.processor(images=image, return_tensors=\"pt\")\n        inputs = {k: v.to(self.device) for k, v in inputs.items()}\n        target_sizes = torch.tensor([image.size[::-1]]).to(self.device)\n        return inputs, target_sizes\n\n    def infer(self, inputs):\n        \"\"\"\n        Performs inference using the DETR model.\n        \n        Args:\n        - inputs (dict): Processed image tensors.\n        \n        Returns:\n        - outputs (dict): Raw model outputs.\n        \"\"\"\n        with torch.no_grad():\n            outputs = self.model(**inputs)\n        return outputs\n\n    def postprocess(self, outputs, target_sizes, threshold=0.5):\n        \"\"\"\n        Extracts object detection results from model outputs.\n        \n        Args:\n        - outputs (dict): Raw model outputs.\n        - target_sizes (torch.Tensor): Image sizes for post-processing.\n        - threshold (float): Confidence threshold for filtering detections.\n        \n        Returns:\n        - results (dict): Processed detection results containing boxes, scores, and labels.\n        \"\"\"\n        results = self.processor.post_process_object_detection(outputs, target_sizes=target_sizes, threshold=threshold)[0]\n        return results\n\n    def plot_predictions(self, image, boxes, scores, labels):\n        \"\"\"\n        Plots the detected bounding boxes on the image.\n        \n        Args:\n        - image (PIL Image): Input image.\n        - boxes (list): Bounding boxes of detected objects.\n        - scores (list): Confidence scores of detected objects.\n        - labels (list): Object class labels.\n        \"\"\"\n        plt.figure(figsize=(16, 10))\n        plt.imshow(image)\n        ax = plt.gca()\n        colors = self.COLORS * 100  # Extend color palette\n\n        for box, score, label, color in zip(boxes, scores, labels, colors):\n            xmin, ymin, xmax, ymax = box\n            ax.add_patch(plt.Rectangle((xmin, ymin), xmax - xmin, ymax - ymin, fill=False, color=color, linewidth=3))\n            text = f\"Golf Ball: {score:.2f}\"\n            ax.text(xmin, ymin, text, fontsize=15, bbox=dict(facecolor='yellow', alpha=0.5))\n\n        plt.axis(\"off\")\n        plt.show()\n\n    def detect(self, image_url, threshold=0.5):\n        \"\"\"\n        Runs the full detection pipeline: image loading, preprocessing, inference, post-processing, and visualization.\n        \n        Args:\n        - image_url (str): URL of the image.\n        - threshold (float): Confidence threshold for filtering detections.\n        \"\"\"\n        print(\"Loading image...\")\n        image = self.load_image(image_url)\n\n        print(\"Preprocessing image...\")\n        inputs, target_sizes = self.preprocess_image(image)\n\n        print(\"Performing inference...\")\n        outputs = self.infer(inputs)\n\n        print(\"Post-processing results...\")\n        results = self.postprocess(outputs, target_sizes, threshold)\n\n        if len(results[\"boxes\"]) == 0:\n            print(\"No objects detected.\")\n        else:\n            # getting bounding boxes, scores, and labels\n            boxes = results[\"boxes\"].tolist()\n            scores = results[\"scores\"].tolist()\n            labels = results[\"labels\"].tolist()\n\n            print(f\"Detected {len(boxes)} objects.\")\n            self.plot_predictions(image, boxes, scores, labels)\n\n        #raw detection results\n        print(\"Boxes:\", results[\"boxes\"])\n        print(\"Scores:\", results[\"scores\"])\n        print(\"Labels:\", results[\"labels\"])\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-30T12:51:14.856430Z","iopub.execute_input":"2024-12-30T12:51:14.856668Z","iopub.status.idle":"2024-12-30T12:51:15.156106Z","shell.execute_reply.started":"2024-12-30T12:51:14.856647Z","shell.execute_reply":"2024-12-30T12:51:15.155157Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Example Usage\nmodel_path = \"./detr_golf_ball\"\nprocessor_path = \"./processor_golf_ball\"\nimage_url = \"https://media.istockphoto.com/id/620963206/ru/%D1%84%D0%BE%D1%82%D0%BE/%D1%8F%D0%BF%D0%BE%D0%BD%D1%81%D0%BA%D0%B0%D1%8F-%D0%B6%D0%B5%D0%BD%D1%89%D0%B8%D0%BD%D0%B0-%D0%B8%D0%B3%D1%80%D0%B0%D1%8E%D1%89%D0%B0%D1%8F-%D0%B2-%D0%B3%D0%BE%D0%BB%D1%8C%D1%84.jpg?s=612x612&w=0&k=20&c=dIrWywIbD9YbCXyHm8N-kV532FJoAOj26Hu4zmTk21g\"\n\ndetector = GolfBallDetector(model_path, processor_path)\ndetector.detect(image_url, threshold=0.5)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-30T12:51:19.685520Z","iopub.execute_input":"2024-12-30T12:51:19.685842Z","iopub.status.idle":"2024-12-30T12:51:19.849904Z","shell.execute_reply.started":"2024-12-30T12:51:19.685816Z","shell.execute_reply":"2024-12-30T12:51:19.849166Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Golf Ball Detection and Trajectory Tracking in Video","metadata":{}},{"cell_type":"markdown","source":"\nThis implementation uses a **DETR (DEtection TRansformer) model** to detect a **golf ball** in a video, track its trajectory, and save the processed video with annotations. The pipeline is structured as a class-based implementation for **modularity** and **reusability**.\n\n---\n\n### **Key Components:**\n\n#### **1. `GolfBallTracker` Class**\n- **`__init__` (Initialization)**  \n  - Loads the **trained DETR model** and **image processor**.\n  - Sets up **input and output video paths**.\n  - Defines **trajectory tracking parameters**, including a **confidence threshold** and **momentum for smoothing movement**.\n\n- **`process_video()` (Main Processing Pipeline)**  \n  - Reads frames from the input video.\n  - Applies **golf ball detection and trajectory tracking** to each frame.\n  - Saves the processed video with bounding boxes and trajectory lines.\n\n- **`detect_and_track()` (Detection & Processing)**  \n  - Converts each frame to **RGB format**.\n  - Runs **inference using the DETR model**.\n  - Extracts **bounding boxes and confidence scores**.\n  - Calls `_handle_detection()` if a golf ball is found or `_handle_prediction()` if not.\n\n- **`_handle_detection()` (Bounding Box Processing)**  \n  - Selects the **best detection** based on confidence scores.\n  - Extracts the **center coordinates** of the detected golf ball.\n  - **Tracks movement** and updates trajectory using **velocity smoothing**.\n  - Draws **bounding boxes and labels** on the frame.\n\n- **`_handle_prediction()` (Extrapolation for Missing Detections)**  \n  - If the golf ball is **not detected**, estimates its **next position** using the **previous velocity**.\n\n- **`_draw_trajectory()` (Visualizing the Ball's Path)**  \n  - Connects past detected points to **draw a trajectory line** on the frame.\n\n---\n\n### **2. Video Processing Pipeline**\n1. **Load Video**  \n   - Reads frames from the input video using OpenCV (`cv2.VideoCapture`).\n   \n2. **Perform Object Detection**  \n   - Uses DETR to **detect the golf ball** in each frame.\n   \n3. **Trajectory Tracking**  \n   - Maintains a **list of previous positions**.\n   - Uses **velocity-based motion estimation** for tracking.\n   \n4. **Draw Bounding Boxes & Trajectory**  \n   - Annotates the **golf ball’s position**.\n   - Draws a **trailing trajectory** to visualize movement.\n   \n5. **Save Processed Video**  \n   - Writes the annotated frames into an **output video file**.\n\n---\n","metadata":{}},{"cell_type":"code","source":"\n\nclass GolfBallTracker:\n    def __init__(self, model_path, processor_path, input_video, output_video, threshold=0.3, momentum=0.8, device=None):\n        \"\"\"\n        Initializes the golf ball tracker for object detection and trajectory tracking.\n\n        Args:\n        - model_path (str): Path to the trained DETR model.\n        - processor_path (str): Path to the pre-trained processor.\n        - input_video (str): Path to the input video file.\n        - output_video (str): Path to save the processed output video.\n        - threshold (float): Confidence threshold for object detection.\n        - momentum (float): Momentum factor for trajectory smoothing.\n        - device (str, optional): Device to use ('cuda' or 'cpu'). If None, it is automatically detected.\n        \"\"\"\n        self.device = device if device else (\"cuda\" if torch.cuda.is_available() else \"cpu\")\n        self.threshold = threshold\n        self.momentum = momentum\n\n        # Load the DETR model and processor\n        self.model = DetrForObjectDetection.from_pretrained(model_path).to(self.device)\n        self.processor = DetrImageProcessor.from_pretrained(processor_path)\n        self.model.eval()\n\n        # Video paths\n        self.input_video = input_video\n        self.output_video = output_video\n\n        # Trajectory tracking variables\n        self.trajectory_points = []\n        self.last_position = None\n        self.velocity = None\n\n    def process_video(self):\n        \"\"\"Processes the input video, detects the golf ball, tracks its trajectory, and saves the output.\"\"\"\n        # Open video file\n        cap = cv2.VideoCapture(self.input_video)\n        if not cap.isOpened():\n            print(\"Error: Could not open video.\")\n            return\n\n        # Video writer setup\n        frame_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n        frame_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n        fps = cap.get(cv2.CAP_PROP_FPS)\n        fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n        out = cv2.VideoWriter(self.output_video, fourcc, fps, (frame_width, frame_height))\n\n        print(\"Processing video...\")\n        for _ in tqdm(range(int(cap.get(cv2.CAP_PROP_FRAME_COUNT))), desc=\"Processing Video\"):\n            ret, frame = cap.read()\n            if not ret:\n                break\n\n            # Detect golf ball\n            self.detect_and_track(frame)\n\n            # Write the processed frame to output\n            out.write(frame)\n\n        # Release resources\n        cap.release()\n        out.release()\n        print(f\"Processed video saved as {self.output_video}.\")\n\n    def detect_and_track(self, frame):\n        \"\"\"\n        Detects the golf ball in the given frame and updates trajectory tracking.\n\n        Args:\n        - frame (numpy array): The input frame.\n        \"\"\"\n        # Convert frame to RGB for model input\n        frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n        inputs = self.processor(images=frame_rgb, return_tensors=\"pt\").to(self.device)\n\n        # Run inference\n        with torch.no_grad():\n            outputs = self.model(**inputs)\n\n        # Post-process predictions\n        target_sizes = torch.tensor([frame.shape[:2][::-1]], device=self.device)\n        results = self.processor.post_process_object_detection(outputs, target_sizes=target_sizes, threshold=self.threshold)[0]\n\n        # Extract bounding boxes and confidence scores\n        boxes = results[\"boxes\"].cpu().numpy() if \"boxes\" in results else []\n        scores = results[\"scores\"].cpu().numpy() if \"scores\" in results else []\n\n        # Handle detections\n        if len(boxes) > 0:\n            self._handle_detection(frame, boxes, scores)\n        else:\n            self._handle_prediction()\n\n        # Draw trajectory on frame\n        self._draw_trajectory(frame)\n\n    def _handle_detection(self, frame, boxes, scores):\n        \"\"\"\n        Handles detection logic when the model successfully detects a golf ball.\n\n        Args:\n        - frame (numpy array): The frame where detection occurs.\n        - boxes (numpy array): Array of bounding boxes.\n        - scores (numpy array): Array of confidence scores.\n        \"\"\"\n        best_idx = np.argmax(scores)\n        if scores[best_idx] >= self.threshold:\n            best_box = boxes[best_idx]\n            x_min, y_min, x_max, y_max = map(int, best_box)\n            center_x, center_y = (x_min + x_max) // 2, (y_min + y_max) // 2\n\n            # Update velocity for trajectory smoothing\n            if self.last_position is not None:\n                dx = center_x - self.last_position[0]\n                dy = center_y - self.last_position[1]\n                self.velocity = (\n                    int(self.momentum * self.velocity[0] + (1 - self.momentum) * dx) if self.velocity else dx,\n                    int(self.momentum * self.velocity[1] + (1 - self.momentum) * dy) if self.velocity else dy,\n                )\n            else:\n                self.velocity = (0, 0)\n\n            self.last_position = (center_x, center_y)\n            self.trajectory_points.append(self.last_position)\n\n            # Draw bounding box and label\n            cv2.rectangle(frame, (x_min, y_min), (x_max, y_max), (0, 255, 0), 2)\n            cv2.putText(frame, f\"Golf Ball: {scores[best_idx]:.2f}\", (x_min, y_min - 10),\n                        cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)\n\n    def _handle_prediction(self):\n        \"\"\"\n        Handles missing detections by estimating the next golf ball position using velocity-based extrapolation.\n        \"\"\"\n        if self.velocity is not None and self.last_position is not None:\n            predicted_x = self.last_position[0] + self.velocity[0]\n            predicted_y = self.last_position[1] + self.velocity[1]\n            self.last_position = (predicted_x, predicted_y)\n            self.trajectory_points.append(self.last_position)\n\n    def _draw_trajectory(self, frame):\n        \"\"\"\n        Draws the detected trajectory of the golf ball on the frame.\n\n        Args:\n        - frame (numpy array): The frame where the trajectory is drawn.\n        \"\"\"\n        for i in range(1, len(self.trajectory_points)):\n            cv2.line(frame, self.trajectory_points[i - 1], self.trajectory_points[i], (0, 255, 255), 2)\n\n# Example Usage\nmodel_path = \"/kaggle/input/transformerdata\"\nprocessor_path = \"/kaggle/working/preprocessor_config.json\"\ninput_video = \"/kaggle/working/output45.mp4\"\noutput_video = \"/kaggle/working/output_with_trajectory.mp4\"\n\ntracker = GolfBallTracker(model_path, processor_path, input_video, output_video)\ntracker.process_video()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-30T12:51:34.510247Z","iopub.execute_input":"2024-12-30T12:51:34.510653Z","iopub.status.idle":"2024-12-30T12:51:34.620884Z","shell.execute_reply.started":"2024-12-30T12:51:34.510616Z","shell.execute_reply":"2024-12-30T12:51:34.620027Z"}},"outputs":[],"execution_count":null}]}